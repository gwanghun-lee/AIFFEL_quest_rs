{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "transformer에서 바뀐 내용\n",
        "- Encoder, Cross-Attention 모두 삭제 → Decoder-only 스택만 유지\n",
        "\n",
        "- PositionalEncoding(sinusoidal) → 학습형 pos embedding\n",
        "\n",
        "- ReLU → GELU\n",
        "\n",
        "- Post-LN(잔차 후 Norm) → Pre-LN(Norm 후 서브레이어, 잔차) + 최종 LayerNorm\n",
        "\n",
        "- 출력은 LM head 하나, weight tying 옵션 **적용**\n",
        "\n",
        "- 학습은 언어모델링 목표 + Causal mask로 통일"
      ],
      "metadata": {
        "id": "8Vq77eBNdzoi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "m730AD23Zo9E"
      },
      "outputs": [],
      "source": [
        "!pip -q install sentencepiece torchinfo\n",
        "\n",
        "import os, re, math, random, zipfile, urllib.request, io\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchinfo import summary\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfkPbxFd-2Rj",
        "outputId": "9f5e9d51-9df9-4d03-fc6b-72357485b215"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip'\n",
        "zip_filename = 'cornell_movie_dialogs.zip'\n",
        "\n",
        "if not os.path.exists(zip_filename):\n",
        "    print(\"Downloading dataset ...\")\n",
        "    urllib.request.urlretrieve(url, zip_filename)\n",
        "    print(\"Done.\")\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zf:\n",
        "    zf.extractall()\n",
        "\n",
        "path_to_dataset = os.path.join(\"cornell movie-dialogs corpus\")\n",
        "path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\n",
        "path_to_movie_conversations = os.path.join(path_to_dataset, 'movie_conversations.txt')\n",
        "\n",
        "print(path_to_dataset)\n",
        "print(os.path.exists(path_to_movie_lines), os.path.exists(path_to_movie_conversations))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHziS83z-4l5",
        "outputId": "a59f7c18-695e-47d9-ec22-903360be06aa"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cornell movie-dialogs corpus\n",
            "True True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence: str) -> str:\n",
        "    s = sentence.lower().strip()\n",
        "    s = re.sub(r\"([?.!,])\", r\" \\1 \", s)\n",
        "    s = re.sub(r'[\" \"]+', \" \", s)\n",
        "    s = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def read_cornell_data(path_to_movie_lines, path_to_movie_conversations, max_samples=50000):\n",
        "    id2line = {}\n",
        "    with open(path_to_movie_lines, 'r', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\" +++$+++ \")\n",
        "            if len(parts) >= 5:\n",
        "                id2line[parts[0]] = parts[4]\n",
        "\n",
        "    pairs = []\n",
        "    with open(path_to_movie_conversations, 'r', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\" +++$+++ \")\n",
        "            if len(parts) < 4:\n",
        "                continue\n",
        "            conv_list = parts[3][1:-1].split(\", \")\n",
        "            conv_list = [c.strip(\"'\") for c in conv_list]\n",
        "            for i in range(len(conv_list) - 1):\n",
        "                q = preprocess_sentence(id2line.get(conv_list[i], \"\"))\n",
        "                a = preprocess_sentence(id2line.get(conv_list[i+1], \"\"))\n",
        "                if q and a:\n",
        "                    pairs.append((q, a))\n",
        "                    if len(pairs) >= max_samples:\n",
        "                        return pairs\n",
        "    return pairs\n",
        "\n",
        "MAX_SAMPLES = 50000\n",
        "pairs = read_cornell_data(path_to_movie_lines, path_to_movie_conversations, MAX_SAMPLES)\n",
        "len(pairs), pairs[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXiH42Jw-_n6",
        "outputId": "0eda691a-91b9-4ae6-8660-c9b6b6179aff"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000,\n",
              " ('can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad . again .',\n",
              "  'well , i thought we d start with pronunciation , if that s okay with you .'))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence: str) -> str:\n",
        "    s = sentence.lower().strip()\n",
        "    s = re.sub(r\"([?.!,])\", r\" \\1 \", s)\n",
        "    s = re.sub(r'[\" \"]+', \" \", s)\n",
        "    s = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def read_cornell_data(path_to_movie_lines, path_to_movie_conversations, max_samples=50000):\n",
        "    id2line = {}\n",
        "    with open(path_to_movie_lines, 'r', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\" +++$+++ \")\n",
        "            if len(parts) >= 5:\n",
        "                id2line[parts[0]] = parts[4]\n",
        "\n",
        "    pairs = []\n",
        "    with open(path_to_movie_conversations, 'r', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\" +++$+++ \")\n",
        "            if len(parts) < 4:\n",
        "                continue\n",
        "            conv_list = parts[3][1:-1].split(\", \")\n",
        "            conv_list = [c.strip(\"'\") for c in conv_list]\n",
        "            for i in range(len(conv_list) - 1):\n",
        "                q = preprocess_sentence(id2line.get(conv_list[i], \"\"))\n",
        "                a = preprocess_sentence(id2line.get(conv_list[i+1], \"\"))\n",
        "                if q and a:\n",
        "                    pairs.append((q, a))\n",
        "                    if len(pairs) >= max_samples:\n",
        "                        return pairs\n",
        "    return pairs\n",
        "\n",
        "MAX_SAMPLES = 50000\n",
        "pairs = read_cornell_data(path_to_movie_lines, path_to_movie_conversations, MAX_SAMPLES)\n",
        "len(pairs), pairs[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ9GtlhM_OIP",
        "outputId": "33e20756-ad57-4013-a89f-0091ecdfbe65"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000,\n",
              " ('can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad . again .',\n",
              "  'well , i thought we d start with pronunciation , if that s okay with you .'))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "VOCAB_SIZE = 8000\n",
        "SPM_PREFIX = \"cornell_bpe\"\n",
        "\n",
        "# 학습용 코퍼스 파일 생성\n",
        "corpus_path = \"spm_corpus.txt\"\n",
        "if not os.path.exists(corpus_path):\n",
        "    with open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for q, a in pairs:\n",
        "            f.write(q + \"\\n\")\n",
        "            f.write(a + \"\\n\")\n",
        "\n",
        "# 모델 학습 (pad/bos/eos/unk id 고정)\n",
        "if not os.path.exists(SPM_PREFIX + \".model\"):\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        input=corpus_path,\n",
        "        model_prefix=SPM_PREFIX,\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        model_type=\"bpe\",\n",
        "        character_coverage=1.0,\n",
        "        pad_id=0, pad_piece=\"<pad>\",\n",
        "        bos_id=1, bos_piece=\"<s>\",\n",
        "        eos_id=2, eos_piece=\"</s>\",\n",
        "        unk_id=3, unk_piece=\"<unk>\"\n",
        "    )\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(SPM_PREFIX + \".model\")\n",
        "\n",
        "PAD_ID = sp.pad_id()\n",
        "BOS_ID = sp.bos_id()\n",
        "EOS_ID = sp.eos_id()\n",
        "sp.GetPieceSize(), PAD_ID, BOS_ID, EOS_ID\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjBEMbyB_Rdo",
        "outputId": "b326a5be-d2f1-4b98-edbe-499a3d644a68"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 0, 1, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LMDataset(Dataset):\n",
        "    def __init__(self, pairs, tokenizer, max_seq_len=128, bos_id=None, eos_id=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.bos_id = tokenizer.bos_id() if bos_id is None else bos_id\n",
        "        self.eos_id = tokenizer.eos_id() if eos_id is None else eos_id\n",
        "        self.examples = []\n",
        "        for q, a in pairs:\n",
        "            ids = [self.bos_id] + tokenizer.encode(q) + [self.eos_id] + tokenizer.encode(a) + [self.eos_id]\n",
        "            if len(ids) >= 2:\n",
        "                self.examples.append(ids[:max_seq_len])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.examples[idx], dtype=torch.long)\n",
        "\n",
        "def lm_collate_fn(batch, pad_id):\n",
        "    max_len = max(len(x) for x in batch)\n",
        "    xs, ys = [], []\n",
        "    for x in batch:\n",
        "        x = torch.tensor(x, dtype=torch.long)\n",
        "        pad_len = max_len - x.size(0)\n",
        "        x = torch.cat([x, torch.full((pad_len,), pad_id, dtype=torch.long)], dim=0)\n",
        "        xs.append(x[:-1])\n",
        "        ys.append(x[1:])\n",
        "    return torch.stack(xs, 0), torch.stack(ys, 0)\n",
        "\n",
        "# 미니 테스트\n",
        "tmp_ds = LMDataset(pairs[:10], sp, max_seq_len=32)\n",
        "tmp_loader = DataLoader(tmp_ds, batch_size=2, collate_fn=lambda b: lm_collate_fn(b, PAD_ID))\n",
        "xb, yb = next(iter(tmp_loader))\n",
        "xb.shape, yb.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYhVOHFU_Uv7",
        "outputId": "60be21b0-8cfe-43b0-e489-eaf3d3832a83"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-704760704.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.long)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 31]), torch.Size([2, 31]))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== GPT1: Decoder-only Transformer =====\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model=768, n_head=12, attn_p=0.1, resid_p=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_head == 0\n",
        "        self.n_head = n_head\n",
        "        self.head_dim = d_model // n_head\n",
        "        self.qkv = nn.Linear(d_model, 3*d_model, bias=True)\n",
        "        self.proj = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.attn_drop = nn.Dropout(attn_p)\n",
        "        self.resid_drop = nn.Dropout(resid_p)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.qkv(x).view(B, T, 3, self.n_head, self.head_dim).permute(2,0,3,1,4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # (B, nh, T, hd)\n",
        "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, nh, T, T)\n",
        "\n",
        "        # causal mask\n",
        "        causal = torch.tril(torch.ones(T, T, device=x.device)).bool().unsqueeze(0).unsqueeze(0)  # (1,1,T,T)\n",
        "        att = att.masked_fill(~causal, float('-inf'))\n",
        "\n",
        "        # optional padding mask (B,1,1,T) where True(mask)=ignore\n",
        "        if attn_mask is not None:\n",
        "            att = att.masked_fill(attn_mask, float('-inf'))\n",
        "\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1,2).contiguous().view(B, T, C)\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, d_model=768, mlp_dim=3072, p=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_dim, d_model),\n",
        "            nn.Dropout(p)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class GPTBlock(nn.Module):\n",
        "    def __init__(self, d_model=768, n_head=12, p=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model, eps=1e-5)\n",
        "        self.attn = CausalSelfAttention(d_model, n_head, p, p)\n",
        "        self.ln2 = nn.LayerNorm(d_model, eps=1e-5)\n",
        "        self.mlp  = MLP(d_model, 4*d_model, p)\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        x = x + self.attn(self.ln1(x), attn_mask=attn_mask)\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPT1(nn.Module):\n",
        "    def __init__(self, vocab_size, n_layer=12, d_model=768, n_head=12, block_size=512, p=0.1, tie_weights=True):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(block_size, d_model)  # learned pos emb\n",
        "        self.drop = nn.Dropout(p)\n",
        "        self.blocks = nn.ModuleList([GPTBlock(d_model, n_head, p) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(d_model, eps=1e-5)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        if tie_weights:\n",
        "            self.head.weight = self.tok_emb.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Linear, nn.Embedding)):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, idx, targets=None, pad_id=None):\n",
        "        B, T = idx.size()\n",
        "        if T > self.block_size:\n",
        "            raise ValueError(f\"seq len {T} > block_size {self.block_size}\")\n",
        "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
        "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        # padding mask for attention: True means \"mask\"\n",
        "        attn_mask = None\n",
        "        if pad_id is not None:\n",
        "            attn_mask = (idx == pad_id).unsqueeze(1).unsqueeze(2)  # (B,1,1,T)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, attn_mask=attn_mask)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            ignore = -100 if pad_id is None else pad_id\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=ignore)\n",
        "        return logits, loss\n"
      ],
      "metadata": {
        "id": "y52n68Gt_XgD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 스모크 테스트용(빠르게) ---\n",
        "BLOCK_SIZE = 128\n",
        "N_LAYER    = 4\n",
        "D_MODEL    = 256\n",
        "N_HEAD     = 8\n",
        "DROPOUT    = 0.1\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# --- 평가 스펙(GPT-1) 예시 ---\n",
        "# BLOCK_SIZE = 512; N_LAYER=12; D_MODEL=768; N_HEAD=12; DROPOUT=0.1; BATCH_SIZE=64\n",
        "\n",
        "gpt = GPT1(\n",
        "    vocab_size=sp.GetPieceSize(),\n",
        "    n_layer=N_LAYER,\n",
        "    d_model=D_MODEL,\n",
        "    n_head=N_HEAD,\n",
        "    block_size=BLOCK_SIZE,\n",
        "    p=DROPOUT,\n",
        "    tie_weights=True\n",
        ").to(device)\n",
        "\n",
        "print(f\"Params: {sum(p.numel() for p in gpt.parameters())/1e6:.2f}M\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeK-6bOE_bu4",
        "outputId": "449e07a8-0781-4fac-e1fa-a65a7aa56161"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: 5.24M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_lr = 6e-4\n",
        "optimizer = AdamW(gpt.parameters(), lr=base_lr, betas=(0.9, 0.999), weight_decay=0.01)\n",
        "\n",
        "def build_cosine_scheduler(optimizer, warmup_steps=2000, total_steps=200000, min_lr=0.0):\n",
        "    def lr_lambda(step):\n",
        "        step = max(step, 1)\n",
        "        if step < warmup_steps:\n",
        "            return step / float(warmup_steps)\n",
        "        progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "        cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "        return max(min_lr / base_lr, cosine)\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "scheduler = build_cosine_scheduler(optimizer, warmup_steps=2000, total_steps=200000)\n"
      ],
      "metadata": {
        "id": "ICHhYjDZ_fpr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = LMDataset(pairs, sp, max_seq_len=BLOCK_SIZE)\n",
        "train_loader  = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                           collate_fn=lambda b: lm_collate_fn(b, PAD_ID), drop_last=True)\n",
        "\n",
        "xb, yb = next(iter(train_loader))\n",
        "print(\"input:\", xb.shape, \"target:\", yb.shape)\n",
        "print(\"shift check:\", xb[0,1:10].tolist(), \"==\", yb[0,:9].tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKENi3m4_jB-",
        "outputId": "48a30d0a-b3fb-4c76-e45c-779ddf0a8204"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: torch.Size([64, 98]) target: torch.Size([64, 98])\n",
            "shift check: [974, 1340, 227, 160, 5, 349, 1756, 1260, 4067] == [974, 1340, 227, 160, 5, 349, 1756, 1260, 4067]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-704760704.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.long)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(gpt, input_size=(1, BLOCK_SIZE-1), dtypes=[torch.long])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_WoA7we_mvz",
        "outputId": "bd95946f-ea7c-4be6-ff3d-8046e5dd7de3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "GPT1                                          [1, 127, 8000]            --\n",
              "├─Embedding: 1-1                              [1, 127, 256]             2,048,000\n",
              "├─Embedding: 1-2                              [1, 127, 256]             32,768\n",
              "├─Dropout: 1-3                                [1, 127, 256]             --\n",
              "├─ModuleList: 1-4                             --                        --\n",
              "│    └─GPTBlock: 2-1                          [1, 127, 256]             --\n",
              "│    │    └─LayerNorm: 3-1                    [1, 127, 256]             512\n",
              "│    │    └─CausalSelfAttention: 3-2          [1, 127, 256]             263,168\n",
              "│    │    └─LayerNorm: 3-3                    [1, 127, 256]             512\n",
              "│    │    └─MLP: 3-4                          [1, 127, 256]             525,568\n",
              "│    └─GPTBlock: 2-2                          [1, 127, 256]             --\n",
              "│    │    └─LayerNorm: 3-5                    [1, 127, 256]             512\n",
              "│    │    └─CausalSelfAttention: 3-6          [1, 127, 256]             263,168\n",
              "│    │    └─LayerNorm: 3-7                    [1, 127, 256]             512\n",
              "│    │    └─MLP: 3-8                          [1, 127, 256]             525,568\n",
              "│    └─GPTBlock: 2-3                          [1, 127, 256]             --\n",
              "│    │    └─LayerNorm: 3-9                    [1, 127, 256]             512\n",
              "│    │    └─CausalSelfAttention: 3-10         [1, 127, 256]             263,168\n",
              "│    │    └─LayerNorm: 3-11                   [1, 127, 256]             512\n",
              "│    │    └─MLP: 3-12                         [1, 127, 256]             525,568\n",
              "│    └─GPTBlock: 2-4                          [1, 127, 256]             --\n",
              "│    │    └─LayerNorm: 3-13                   [1, 127, 256]             512\n",
              "│    │    └─CausalSelfAttention: 3-14         [1, 127, 256]             263,168\n",
              "│    │    └─LayerNorm: 3-15                   [1, 127, 256]             512\n",
              "│    │    └─MLP: 3-16                         [1, 127, 256]             525,568\n",
              "├─LayerNorm: 1-5                              [1, 127, 256]             512\n",
              "├─Linear: 1-6                                 [1, 127, 8000]            2,048,000\n",
              "===============================================================================================\n",
              "Total params: 7,288,320\n",
              "Trainable params: 7,288,320\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 7.29\n",
              "===============================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 20.35\n",
              "Params size (MB): 29.15\n",
              "Estimated Total Size (MB): 49.51\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from contextlib import nullcontext\n",
        "\n",
        "def train_epoch(\n",
        "    model, loader, optimizer, scheduler, device, pad_id,\n",
        "    grad_accum_steps: int = 1,     # 유효 배치 키우고 싶을 때 >1로\n",
        "    use_amp: bool = True,          # CUDA일 때 자동혼합정밀도 사용\n",
        "    log_every: int = 100\n",
        "):\n",
        "    model.train()\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(use_amp and device.type == \"cuda\"))\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_tok  = 0\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for step, (inp, tgt) in enumerate(loader, start=1):\n",
        "        inp = inp.to(device, non_blocking=True)\n",
        "        tgt = tgt.to(device, non_blocking=True)\n",
        "\n",
        "        ctx = torch.cuda.amp.autocast(enabled=(scaler.is_enabled()))\n",
        "        with ctx:\n",
        "            # next-token LM loss (PAD 무시)\n",
        "            _, loss = model(inp, targets=tgt, pad_id=pad_id)\n",
        "            # gradient accumulation 시 손실을 나눠서 스케일 유지\n",
        "            loss_for_backward = loss / grad_accum_steps\n",
        "\n",
        "        # backward\n",
        "        if scaler.is_enabled():\n",
        "            scaler.scale(loss_for_backward).backward()\n",
        "        else:\n",
        "            loss_for_backward.backward()\n",
        "\n",
        "        # optimizer / scheduler step (accumulation 주기마다)\n",
        "        if step % grad_accum_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            if scaler.is_enabled():\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                optimizer.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "        # 통계 (PAD 제외 토큰 수 기준 평균)\n",
        "        ntok = (tgt != pad_id).sum().item()\n",
        "        total_loss += loss.item() * max(ntok, 1)\n",
        "        total_tok  += max(ntok, 1)\n",
        "\n",
        "        if step % log_every == 0:\n",
        "            curr_lr = scheduler.get_last_lr()[0] if scheduler is not None else optimizer.param_groups[0][\"lr\"]\n",
        "            # 로그에는 실제 손실(loss) 출력 (accum 나누기 전 값)\n",
        "            print(f\"[step {step:4d}] loss={loss.item():.4f}  lr={curr_lr:.8f}\")\n",
        "\n",
        "    return total_loss / max(total_tok, 1)\n",
        "\n",
        "# ----- 학습 루프 -----\n",
        "EPOCHS = 10\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    avg = train_epoch(gpt, train_loader, optimizer, scheduler, device, PAD_ID,\n",
        "                      grad_accum_steps=1, use_amp=True, log_every=100)\n",
        "    ppl = math.exp(min(avg, 20))  # overflow 방지\n",
        "    print(f\"[Epoch {ep}] avg_token_loss={avg:.4f}  ppl={ppl:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua1Kuzm3_wFx",
        "outputId": "ba2c9c62-e257-4eb1-96e5-73e383316d50"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2600375405.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(use_amp and device.type == \"cuda\"))\n",
            "/tmp/ipython-input-704760704.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.long)\n",
            "/tmp/ipython-input-2600375405.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  ctx = torch.cuda.amp.autocast(enabled=(scaler.is_enabled()))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[step  100] loss=3.8064  lr=0.00059868\n",
            "[step  200] loss=3.6714  lr=0.00059864\n",
            "[step  300] loss=3.8123  lr=0.00059859\n",
            "[step  400] loss=3.5980  lr=0.00059854\n",
            "[step  500] loss=3.6293  lr=0.00059850\n",
            "[step  600] loss=3.8236  lr=0.00059845\n",
            "[step  700] loss=3.7628  lr=0.00059840\n",
            "[Epoch 1] avg_token_loss=3.7331  ppl=41.81\n",
            "[step  100] loss=3.6507  lr=0.00059831\n",
            "[step  200] loss=3.8333  lr=0.00059826\n",
            "[step  300] loss=3.6719  lr=0.00059821\n",
            "[step  400] loss=3.4927  lr=0.00059816\n",
            "[step  500] loss=3.8156  lr=0.00059810\n",
            "[step  600] loss=3.5673  lr=0.00059805\n",
            "[step  700] loss=3.6043  lr=0.00059799\n",
            "[Epoch 2] avg_token_loss=3.6749  ppl=39.45\n",
            "[step  100] loss=3.5703  lr=0.00059789\n",
            "[step  200] loss=3.6324  lr=0.00059784\n",
            "[step  300] loss=3.6922  lr=0.00059778\n",
            "[step  400] loss=3.4600  lr=0.00059772\n",
            "[step  500] loss=3.6262  lr=0.00059766\n",
            "[step  600] loss=3.5728  lr=0.00059760\n",
            "[step  700] loss=3.6511  lr=0.00059754\n",
            "[Epoch 3] avg_token_loss=3.6205  ppl=37.36\n",
            "[step  100] loss=3.5930  lr=0.00059743\n",
            "[step  200] loss=3.4533  lr=0.00059737\n",
            "[step  300] loss=3.5911  lr=0.00059731\n",
            "[step  400] loss=3.5097  lr=0.00059724\n",
            "[step  500] loss=3.4600  lr=0.00059718\n",
            "[step  600] loss=3.5940  lr=0.00059711\n",
            "[step  700] loss=3.6691  lr=0.00059705\n",
            "[Epoch 4] avg_token_loss=3.5687  ppl=35.47\n",
            "[step  100] loss=3.6383  lr=0.00059692\n",
            "[step  200] loss=3.5350  lr=0.00059685\n",
            "[step  300] loss=3.6297  lr=0.00059679\n",
            "[step  400] loss=3.6866  lr=0.00059672\n",
            "[step  500] loss=3.4504  lr=0.00059665\n",
            "[step  600] loss=3.4896  lr=0.00059657\n",
            "[step  700] loss=3.3817  lr=0.00059650\n",
            "[Epoch 5] avg_token_loss=3.5193  ppl=33.76\n",
            "[step  100] loss=3.3128  lr=0.00059637\n",
            "[step  200] loss=3.4720  lr=0.00059630\n",
            "[step  300] loss=3.4852  lr=0.00059622\n",
            "[step  400] loss=3.2851  lr=0.00059614\n",
            "[step  500] loss=3.3506  lr=0.00059607\n",
            "[step  600] loss=3.5381  lr=0.00059599\n",
            "[step  700] loss=3.4858  lr=0.00059591\n",
            "[Epoch 6] avg_token_loss=3.4723  ppl=32.21\n",
            "[step  100] loss=3.3232  lr=0.00059577\n",
            "[step  200] loss=3.4634  lr=0.00059569\n",
            "[step  300] loss=3.3183  lr=0.00059561\n",
            "[step  400] loss=3.3849  lr=0.00059553\n",
            "[step  500] loss=3.4657  lr=0.00059545\n",
            "[step  600] loss=3.3714  lr=0.00059536\n",
            "[step  700] loss=3.5453  lr=0.00059528\n",
            "[Epoch 7] avg_token_loss=3.4273  ppl=30.79\n",
            "[step  100] loss=3.3605  lr=0.00059513\n",
            "[step  200] loss=3.2296  lr=0.00059504\n",
            "[step  300] loss=3.4050  lr=0.00059495\n",
            "[step  400] loss=3.3713  lr=0.00059487\n",
            "[step  500] loss=3.4168  lr=0.00059478\n",
            "[step  600] loss=3.2386  lr=0.00059469\n",
            "[step  700] loss=3.4108  lr=0.00059460\n",
            "[Epoch 8] avg_token_loss=3.3826  ppl=29.45\n",
            "[step  100] loss=3.2935  lr=0.00059444\n",
            "[step  200] loss=3.3020  lr=0.00059434\n",
            "[step  300] loss=3.3887  lr=0.00059425\n",
            "[step  400] loss=3.4248  lr=0.00059416\n",
            "[step  500] loss=3.3610  lr=0.00059406\n",
            "[step  600] loss=3.3004  lr=0.00059397\n",
            "[step  700] loss=3.3441  lr=0.00059387\n",
            "[Epoch 9] avg_token_loss=3.3419  ppl=28.27\n",
            "[step  100] loss=3.3346  lr=0.00059370\n",
            "[step  200] loss=3.2554  lr=0.00059360\n",
            "[step  300] loss=3.2781  lr=0.00059350\n",
            "[step  400] loss=3.3310  lr=0.00059341\n",
            "[step  500] loss=3.4065  lr=0.00059331\n",
            "[step  600] loss=3.5358  lr=0.00059321\n",
            "[step  700] loss=3.3005  lr=0.00059310\n",
            "[Epoch 10] avg_token_loss=3.3015  ppl=27.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, tokenizer, prompt, max_new_tokens=40, temperature=1.0, top_k=50, top_p=None, device='cpu'):\n",
        "    model.eval()\n",
        "    text = preprocess_sentence(prompt)\n",
        "    ids = [tokenizer.bos_id()] + tokenizer.encode(text)\n",
        "    x = torch.tensor([ids], dtype=torch.long, device=device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if x.size(1) > model.block_size:\n",
        "            x = x[:, -model.block_size:]\n",
        "        logits, _ = model(x)\n",
        "        logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        if top_k is not None:\n",
        "            v, ix = torch.topk(probs, top_k, dim=-1)\n",
        "            mask = torch.ones_like(probs, dtype=torch.bool).scatter(1, ix, False)\n",
        "            probs = probs.masked_fill(mask, 0)\n",
        "            probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "        if top_p is not None:\n",
        "            sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
        "            cum = torch.cumsum(sorted_probs, dim=-1)\n",
        "            mask = cum > top_p\n",
        "            mask[..., 0] = False\n",
        "            sorted_probs = sorted_probs.masked_fill(mask, 0)\n",
        "            probs = torch.zeros_like(probs).scatter(1, sorted_idx, sorted_probs)\n",
        "            probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "        next_id = torch.multinomial(probs, num_samples=1)\n",
        "        x = torch.cat([x, next_id], dim=1)\n",
        "        if next_id.item() == tokenizer.eos_id():\n",
        "            break\n",
        "\n",
        "    out = x[0].tolist()\n",
        "    out = [t for t in out if t not in (tokenizer.bos_id(), tokenizer.eos_id(), tokenizer.pad_id())]\n",
        "    return tokenizer.decode(out)\n",
        "\n",
        "print(generate(gpt, sp, \"Where have you been?\", device=device))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_R0oO9QAiyM",
        "outputId": "c5c15b0e-21f8-4d9f-8a85-c11778b9ad4b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "where have you been ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === GPT-1용 챗봇 생성 함수 (seq2seq 인퍼런스 대체 버전) ===\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def decoder_inference_gpt1(model, sentence, tokenizer, device='cpu',\n",
        "                           max_new_tokens=40, temperature=1.0, top_k=50, top_p=None):\n",
        "    \"\"\"\n",
        "    - seq2seq의 enc/dec 입력을 쓰지 않고, GPT-1처럼 '프롬프트 이어쓰기'로 생성\n",
        "    - 반환: 프롬프트 이후에 생성된 토큰 id 리스트\n",
        "    \"\"\"\n",
        "    BOS = tokenizer.bos_id()\n",
        "    EOS = tokenizer.eos_id()\n",
        "    PAD = tokenizer.pad_id()\n",
        "\n",
        "    # 전처리 후 프롬프트 토크나이즈 (BOS만 앞에 붙임)\n",
        "    prompt = preprocess_sentence(sentence)\n",
        "    prompt_ids = [BOS] + tokenizer.encode(prompt)\n",
        "\n",
        "    # (1, T) 텐서\n",
        "    x = torch.tensor([prompt_ids], dtype=torch.long, device=device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            # 컨텍스트 길이 제한 (모델에 block_size 속성이 없으면 1024로 가정)\n",
        "            max_ctx = getattr(model, 'block_size', 1024)\n",
        "            if x.size(1) > max_ctx:\n",
        "                x = x[:, -max_ctx:]\n",
        "\n",
        "            # 마지막 토큰의 로짓으로 다음 토큰 샘플링\n",
        "            logits, _ = model(x)                    # (B, T, V)\n",
        "            logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Top-k 필터링\n",
        "            if top_k is not None:\n",
        "                k = min(top_k, probs.size(-1))\n",
        "                v, ix = torch.topk(probs, k, dim=-1)\n",
        "                mask = torch.ones_like(probs, dtype=torch.bool).scatter(1, ix, False)\n",
        "                probs = probs.masked_fill(mask, 0)\n",
        "                probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "            # Top-p(nucleus) 필터링 (선택)\n",
        "            if top_p is not None:\n",
        "                sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
        "                cum = torch.cumsum(sorted_probs, dim=-1)\n",
        "                mask = cum > top_p\n",
        "                mask[..., 0] = False\n",
        "                sorted_probs = sorted_probs.masked_fill(mask, 0)\n",
        "                probs = torch.zeros_like(probs).scatter(1, sorted_idx, sorted_probs)\n",
        "                probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "            next_id = torch.multinomial(probs, num_samples=1)  # (1,1)\n",
        "            x = torch.cat([x, next_id], dim=1)\n",
        "\n",
        "            # EOS면 조기 종료\n",
        "            if next_id.item() == EOS:\n",
        "                break\n",
        "\n",
        "    # 프롬프트 이후 생성분만 반환\n",
        "    full_ids = x[0].tolist()\n",
        "    gen_ids = full_ids[len(prompt_ids):]\n",
        "    # BOS/PAD 제거\n",
        "    gen_ids = [t for t in gen_ids if t not in (BOS, PAD)]\n",
        "    return gen_ids\n",
        "\n",
        "\n",
        "def sentence_generation_gpt1(model, sentence, tokenizer, device='cpu',\n",
        "                             max_new_tokens=40, temperature=1.0, top_k=50, top_p=None):\n",
        "    \"\"\"\n",
        "    - decoder_inference_gpt1()를 호출해 문장을 생성하고 decode까지 수행\n",
        "    - 출력: 디코딩된 문자열\n",
        "    \"\"\"\n",
        "    out_ids = decoder_inference_gpt1(\n",
        "        model, sentence, tokenizer, device=device,\n",
        "        max_new_tokens=max_new_tokens, temperature=temperature,\n",
        "        top_k=top_k, top_p=top_p\n",
        "    )\n",
        "    text = tokenizer.decode(out_ids) if out_ids else \"\"\n",
        "    print(\"입력 :\", sentence)\n",
        "    print(\"출력 :\", text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "8wzgHcF-DoWi"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Where have you been?\"\n",
        "\n",
        "sentence_generation_gpt1(gpt, sentence, sp, device, max_new_tokens=40, temperature=1.0, top_k=50)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "-MMYhYu5Dqle",
        "outputId": "c48a837e-cb00-4e9e-fe12-6645369d2936"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : Where have you been?\n",
            "출력 : \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ]
}